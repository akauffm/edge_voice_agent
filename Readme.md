# Small Voice Agent running fully offline on edge devices

* flexible wrt to ASR, LLM and TTS component, currently supported:
   * ASR: Moonshine, FasterWhisper, Nemo FastConformer, Vosk
   * TTS: Piper, Kokoro
   * LLM: anything through Ollama
* components chosen to work fully offline on-device, CPU only
   * default setup can run on Raspberry Pi 5
      * ASR: Moonshine tiny
      * TTSL: Piper
      * LLM: Gemma3:1b


## Installation

### Core functionality

* clone git repository
   * ```git clone https://github.com/akauffm/edge_voice_agent && cd edge_voice_agent```

* create a virtual environment and activate it
   * ```python -m venv venv```
   * ```source venv/bin/activate```

* install dependencies
   * ```pip install -r requirements.txt```
   * ```git clone https://github.com/ktomanek/captioning``` 
   * ```cd captioning && pip install -e . && cd ..```
   * ```git clone https://github.com/ktomanek/edge_tts_comparison```
   * ```cd edge_tts_comparison && pip install -e . && cd ..```
   * ```pip install onnxruntime piper-phonemize-cross```
   * ```git clone https://github.com/rhasspy/piper.git```
   * ```cd piper/src/python_run/```
   * ```rm requirements.txt```
   * ```pip install . && cd ../../..```

* download sentence splitter: 
   * ```python -c "import nltk; nltk.download('punkt_tab')"```

* install moonshine (speech-to-text)
   * ```pip install useful-moonshine-onnx@git+https://git@github.com/usefulsensors/moonshine.git#subdirectory=moonshine-onnx```

   * optionally install other models (mostly useful for testing, the defaults already installed are the fastest):

   * ```pip install "nemo_toolkit[asr]"```
   * ```pip install faster whisper```

* basic Piper text-to-speech (TTS) is installed already and includes one voice
   * to install other voices, 
   * to use Kokoro,


#### Ollama

* install ollama locally: https://ollama.com/download
* then pull the model you want to use, eg: 

```ollama pull gemma3:1b```

* then install [ollama python library](https://github.com/ollama/ollama-python) 

```pip install ollama```

* start ollama from its UI or on the command line in a new terminal window

```ollama serve```





### CLI command line arguments


## End of utterance detection

```--end_of_utterance_duration 0.7``` determines when we consider the user input to be finished. Adapt according to user's speaking patterns, slower speakers might need a higher value. ```0.7``` seems to be a good default

## System prompt from text

* you can increase the speaking rate to make long responses not feel quite as long

```python voice_agent_cli.py --speaking_rate 3.0 --system_prompt "`cat examples/cat_specialist.txt`" ```

## Other models

* moonshine base seems to run fast enough on Raspberry Pi.
* Gemma3:4b leads to significant improvement on conversation side, but is too slow on Raspberry Pi

```python voice_agent_cli.py --asr_model_name moonshine_onnx_base --ollama_model_name gemma3:4b --speaking_rate 3.0```


## Performance measurements

### User Speech input

See [here](https://github.com/ktomanek/captioning?tab=readme-ov-file#streaming-performance-comparison) for comparison on different ASR models in the streaming lib on various devices.

### LLM Generation

Before audio output can be generated, the LLM needs to generate enough tokens to start synthesizing audio output.
When running ```voice_agent_cli.py --verbose```, several performance metrics will be shown quantifying this latency.

  * Time to first token (seconds)
      * measures the time it took the LLM until the first token was generated
      * this in only dependent on the LLM's inference speed in streaming mode
  * Time to first speech fragment (seconds)
      * measures how long it took the LLM to generate enough tokens needed to start synthesizing audio output (this doesn't include the time needed to actually generate the audio ouput, but is an important measure for minimal latency until audio can be generated)
      * this also depends on how the parameter ```--max_words_to_speak_start``` is set. A lower number means that the first speech segment is shorter and hence can be generated by the LLM quicker; the downside is a likely more synthetic sounding output. Processing speech on the respecitve device should be taken into consideration here.