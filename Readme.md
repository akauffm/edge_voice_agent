# Small Voice Agent running fully offline on edge devices

* **Currently supports:**
   * ASR: Moonshine, FasterWhisper, Nemo FastConformer, Vosk
   * TTS: Piper, Kokoro
   * LLM: anything through Ollama
* **Default setup works fully offline on-device, CPU only (with ~1s total latency on Raspberry Pi 5):**
   * ASR: Moonshine tiny
   * TTS: Piper
   * LLM: Gemma3:1b


## Installation

* clone git repository
   * ```git clone https://github.com/akauffm/edge_voice_agent && cd edge_voice_agent```

* create a virtual environment and activate it
   * ```python -m venv venv```
   * ```source venv/bin/activate```

* install dependencies
   * ```pip install -r requirements.txt```
   * ```git clone https://github.com/ktomanek/captioning``` 
   * ```cd captioning && pip install -e . && cd ..```
   * ```git clone https://github.com/ktomanek/edge_tts_comparison```
   * ```cd edge_tts_comparison && pip install -e . && cd ..```
   * ```pip install onnxruntime piper-phonemize-cross```
   * ```git clone https://github.com/rhasspy/piper.git```
   * ```cd piper/src/python_run/```
   * ```rm requirements.txt```
   * ```pip install . && cd ../../..```

* download sentence splitter: 
   * ```python -c "import nltk; nltk.download('punkt_tab')"```

* install moonshine (speech-to-text)
   * ```pip install useful-moonshine-onnx@git+https://git@github.com/usefulsensors/moonshine.git#subdirectory=moonshine-onnx```

   * optionally install other models (mostly useful for testing, the defaults already installed are the fastest):
   * ```pip install "nemo_toolkit[asr]"```
   * ```pip install faster whisper```

* basic Piper text-to-speech (TTS) is installed already and includes one voice
   * to install additional Piper voices, visit https://huggingface.co/rhasspy/piper-voices/tree/main and choose the appropriate language. Navigate to the folder of the desired voice and download the **ONNX** and **JSON** file for each into the edge_voice_agent folder. You can hear all the voices at https://rhasspy.github.io/piper-samples/. *Models are all available in medium quality, some also in low and high quality. Inference time is signifiantly faster in low quality.*
   * to use Kokoro, take a look at [this](https://github.com/ktomanek/edge_tts_comparison/blob/main/download_kokoro_models.sh)

* install Ollama for your platform
   * https://ollama.com/download
   * from the command line, pull the model you want to use, eg: ```ollama pull gemma3:1b``` (Ollama models are stored in a folder called .ollama in your home directory. You can see which are installed with ```ollama list``` and can delete them using ```ollama rm [model name]```)

* start ollama from its UI or on the command line in a new terminal window
   * ```ollama serve```

### Phew! If you got this far, you're ready to actually run this bad boy

## Run it!

From the command line, try: ```python voice_agent_cli.py --system_prompt "Answer the user in one sentence. You're a technical expert who's very easily distracted. Answer in one jargon-filled sentence." \
   --ollama_model_name "gemma3:1b" \
   --start_message "I know all about the edge because I live on it! Ask me anything." \
   --tts_model_path "en_US-ryan-low.onnx"\
   --speaking_rate 1.5```

### For the full list of available command line flags, run ```python voice_agent_cli.py -h``` 

### End of utterance detection

```--end_of_utterance_duration 0.7``` determines when we consider the user input to be finished speaking. Slower speakers might need a higher value. ```0.7``` seems to be a good default

### System prompt from text file

If you have a really long prompt, you can pass it as a text file rather than on the command line:
```python voice_agent_cli.py --speaking_rate 3.0 --system_prompt "`cat [path to text file with prompt]`" ```

### LLM generation times

Before audio output can be generated, the LLM needs to generate enough tokens to start synthesizing audio output.
When running ```voice_agent_cli.py --verbose```, several performance metrics will be shown quantifying this latency.

  * Time to first token (seconds)
      * measures the time it took the LLM until the first token was generated
      * this in only dependent on the LLM's inference speed in streaming mode
  * Time to first speech fragment (seconds)
      * measures how long it took the LLM to generate enough tokens needed to start synthesizing audio output (this doesn't include the time needed to actually generate the audio ouput, but is an important measure for minimal latency until audio can be generated)
      * this also depends on how the parameter ```--max_words_to_speak_start``` is set. A lower number means that the first speech segment is shorter and hence can be generated by the LLM quicker; the downside is a likely more synthetic sounding output. Processing speech on the respecitve device should be taken into consideration here.